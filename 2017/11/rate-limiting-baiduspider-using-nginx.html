<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  Rate Limiting Baiduspider Using nginx | Nairobi GNU/Linux Users Group
</title>
  <link rel="canonical" href="/2017/11/rate-limiting-baiduspider-using-nginx.html">

    <link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/monokai.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">

  
  <meta name="description" content="Using nginx maps and limit request zones to rate limit Baiduspider for its disregard of robots.txt, sitemaps, and server resource usage.">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
    <div class="col-sm-4">
      <a href="/">
        <img class="img-fluid rounded" src=/images/profile.png alt="Nairobi GNU/Linux Users Group">
      </a>
    </div>
  <div class="col-sm-8">
    <h1 class="title"><a href="/">Nairobi GNU/Linux Users Group</a></h1>
      <p class="text-muted">A lively community of GNU/Linux enthusiasts</p>
      <ul class="list-inline">
          <li class="list-inline-item"><a href="https://web.libera.chat/?channels=#nairobilug" target="_blank">IRC</a></li>
              <li class="list-inline-item text-muted">|</li>
            <li class="list-inline-item"><a href="/pages/about-us.html">About Us</a></li>
            <li class="list-inline-item"><a href="/pages/code-of-conduct.html">Code of Conduct</a></li>
            <li class="list-inline-item"><a href="/pages/contact.html">Contact</a></li>
            <li class="list-inline-item"><a href="/pages/mailing-list.html">Mailing List</a></li>
            <li class="list-inline-item"><a href="/pages/meetups.html">Meetups</a></li>
            <li class="list-inline-item"><a href="/pages/software.html">Software</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fa fa-github" href="https://github.com/nairobilug" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fa fa-twitter" href="https://twitter.com/nairobilug" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>  Rate Limiting Baiduspider Using nginx
</h1>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2017-11-22T17:42:00+03:00">
          <i class="fa fa-clock-o"></i>
          Wed 22 November 2017
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="/category/linux.html">Linux</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-user-o"></i>
              <a href="/author/alan-orth.html">Alan Orth</a>          </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="/tag/baidu.html">#Baidu</a>,               <a href="/tag/nginx.html">#nginx</a>,               <a href="/tag/performance.html">#Performance</a>          </li>
      </ul>
    </header>
    <div class="content">
      <p>The Baidu search engine has a voracious appetite for content and crawls one of my sites aggressively. It's bad enough having to deal with load generated by bots from large technology companies with vast resources, but it's another thing entirely when those bots crawl from <em>dozens of IP addresses simultaneously</em> and routinely browse thousands of URLs disallowed in my <code>robots.txt</code>. After months of periodic alerts from my server about high resource usage — not to mention complaints from my users about the site being "slow" — I've finally had enough.</p>
<p>I used nginx's <a href="https://nginx.org/en/docs/http/ngx_http_map_module.html"><code>map</code></a> and <a href="https://nginx.org/en/docs/http/ngx_http_limit_req_module.html"><code>limit_req</code></a> modules to selectively apply a rate limit to Baidu's requests and now the server's resource usage has improved. Blocking these requests entirely would have been more simple, but I didn't feel comfortable completely stifling the flow of information, and this powerful, flexible technique could be useful in the future anyways.</p>
<h2>No Respect for robots.txt</h2>
<p>According to the Baiduspider help center <a href="http://www.baidu.com/search/robots_english.html">Baidu respects the robots exclusion protocol</a> — I even found a <a href="http://ziyuan.baidu.com/robots/">testing tool</a> that confirms my <code>robots.txt</code> file is valid. Curiously, my site's daily access log shows a client that identifies itself as "Baiduspider" making 2,500 requests to URLs that are disallowed by my <code>robots.txt</code>:</p>
<div class="highlight"><pre><span></span><code><span class="gp"># </span>cat<span class="w"> </span>/var/log/nginx/access.log<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-c<span class="w"> </span>Baiduspider
<span class="go">8912</span>
<span class="gp"># </span>cat<span class="w"> </span>/var/log/nginx/access.log<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Baiduspider<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>-c<span class="w"> </span>-E<span class="w"> </span><span class="s2">"GET /(browse|discover|search-filter)"</span>
<span class="go">2521</span>
</code></pre></div>
<p>Not that I need a reason, but I designate these URLs as off limits to bots on purpose: they're dynamic search pages with countless permutations of input parameters that generate equally countless pages of results. Crawling these pages adds nothing of value to Baidu's search index and wastes resources on my server in the process. If it's content they want, my <code>robots.txt</code> actually provides a nice, neat sitemap that conveniently lists all content pages in machine-readable XML. Sadly, upon looking at my access logs I see Google, Bing, and Yandex bots each requesting this sitemap several times per day, but Baidu doesn't even request it once.</p>
<p>Baidu doesn't respect my server resources or indexing preferences. This is not responsible harvesting!</p>
<p><em>Note: it might be possible to submit your sitemap to Baidu's webmaster tools — if you can navigate their site in Chinese!</em></p>
<h2>Mapping and Request Limiting</h2>
<p>The nginx <code>map</code> module works similarly to an if–then–else construct, allowing you to <em>set the value of one variable depending on the value of another variable</em>. I will combine this with a clever use of the <code>limit_req</code> module to create a rate limit that only punishes Baidu. Add the following code to the global <code>http</code> block of the nginx configuration (not in a server block):</p>
<div class="highlight"><pre><span></span><code><span class="k">map</span><span class="w"> </span><span class="nv">$http_user_agent</span><span class="w"> </span><span class="nv">$limit_bots</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kn">~Baiduspider</span><span class="w"> </span><span class="s">'baidu'</span><span class="p">;</span>

<span class="w">    </span><span class="c1"># requests with an empty key are not evaluated by limit_req</span>
<span class="w">    </span><span class="c1"># see: http://nginx.org/en/docs/http/ngx_http_limit_req_module.html</span>
<span class="w">    </span><span class="kn">default</span><span class="w"> </span><span class="s">''</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">limit_req_zone</span><span class="w"> </span><span class="nv">$limit_bots</span><span class="w"> </span><span class="s">zone=badbots:1m</span><span class="w"> </span><span class="s">rate=1r/m</span><span class="p">;</span>
</code></pre></div>
<p>This mapping uses the nginx built-in variable <code>$http_user_agent</code> as the source and a new variable <code>$limit_bots</code> as the target. Depending on whether or not the request's user agent matches the regular expression, <code>$limit_bots</code> will either be set to the value "baidu" or, by default, an empty string.</p>
<p>Next, a <code>limit_req_zone</code> called "badbots" is created with a limit of one request per minute. The clever trick here is the use of <code>$limit_bots</code> as the zone's key, because <em>requests with an empty key are not evaluated</em>, and thus not subject to rate limiting.</p>
<p>The last step is to assign this zone to a <code>location</code> block in an nginx <code>server</code> block:</p>
<div class="highlight"><pre><span></span><code><span class="k">location</span><span class="w"> </span><span class="s">/</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1"># rate limit for poorly behaved bots, see limit_req_zone below</span>
<span class="w">    </span><span class="kn">limit_req</span><span class="w"> </span><span class="s">zone=badbots</span><span class="p">;</span>

<span class="w">    </span><span class="c1"># Send requests to Tomcat</span>
<span class="w">    </span><span class="kn">proxy_pass</span><span class="w"> </span><span class="s">http://tomcat_http</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>
<p>After checking the configuration syntax with <code>nginx -t</code> and reloading the daemon with <code>nginx -s reload</code> you will be ready to test the new mapping.</p>
<h2>Test the Configuration</h2>
<p>My favorite tool for testing HTTP requests is the Python-based command line utility <a href="https://httpie.org/">httpie</a>. I find it much easier to set and view request and response headers using httpie than a web browser's developer tools.</p>
<p>To test the new mapping and rate limiting configuration I send two requests to the server while mimicking part of the Baidu bot's user agent:</p>
<div class="highlight"><pre><span></span><code><span class="gp">$ </span>http<span class="w"> </span>--print<span class="w"> </span>h<span class="w"> </span>https://mysite.org/<span class="w"> </span>User-Agent:<span class="s1">'Baiduspider'</span>
<span class="go">HTTP/1.1 200 OK</span>
<span class="gp">$ </span>http<span class="w"> </span>--print<span class="w"> </span>h<span class="w"> </span>https://mysite.org/<span class="w"> </span>User-Agent:<span class="s1">'Baiduspider'</span>
<span class="go">HTTP/1.1 503 Service Temporarily Unavailable</span>
</code></pre></div>
<p>Great success! The first request succeeds with an HTTP 200, and the second fails with an HTTP 503.</p>
<p>You can adjust the configuration further depending on your specific use case, for example adding more patterns to match in the mapping configuration, changing the error response code, or allowing bursts of requests in the <code>limit_req_zone</code>. See the references below for more information.</p>
<h2>References</h2>
<ul>
<li><a href="https://www.nginx.com/blog/rate-limiting-nginx/">Rate Limiting with NGINX and NGINX Plus</a></li>
<li><a href="https://nginx.org/en/docs/http/ngx_http_map_module.html">Module ngx_http_map_module</a></li>
<li><a href="https://nginx.org/en/docs/http/ngx_http_limit_req_module.html">Module ngx_http_limit_req_module</a></li>
</ul>
<p>This was <a href="https://mjanja.ch/2017/11/rate-limiting-baiduspider-using-nginx/">originally posted</a> on my personal blog; re-posted here for posterity.</p>
    </div>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
      <li class="list-inline-item"><a href="/authors.html">Authors</a></li>
    <li class="list-inline-item"><a href="/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="/tags.html">Tags</a></li>
  </ul>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>

</html>